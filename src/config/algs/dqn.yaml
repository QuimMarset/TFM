# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy_single"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000
evaluation_epsilon: 0
lr: 0.001 # Learning rate for agents
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm
add_value_last_step: True

runner: "episode"
mac: "single_mac"

buffer_size: 2000
batch_size: 200 # Number of episodes to train on

# update the target network every {} episodes
target_update_interval_or_tau: 50

save_model: True
save_model_interval: 5000

#checkpoint_path: "D:/MAI/TFM/epymarl-main/src/results/models/dqn_seed859535793_pistonball_reward_2_actions_2022-12-14"
evaluate: False # Evaluate model for test_nepisode episodes and quit (no training)
#load_step: 997800

agent: "linear" # Default rnn agent
obs_agent_id: False # Include the agent's one_hot id in the observation
obs_last_action: False # Include the agent's last action (one_hot) in the observation
obs_individual_obs: False
batch_size_run: 1 # Number of environments to run in parallel

# use the Q_Learner to train
standardise_returns: True
standardise_rewards: True

agent_output_type: "q"
learner: "q_learner_single"
double_q: False
use_rnn: False

hidden_dim: 512

mixer:

use_cuda: False

name: "dqn"