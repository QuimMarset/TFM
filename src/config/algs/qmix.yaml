# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000
evaluation_epsilon: 0
lr: 0.001 # Learning rate for agents
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 100 # Reduce magnitude of gradients above this L2 norm
add_value_last_step: True

runner: "episode"
mac: "shared_but_sides_mac"

buffer_size: 2000
batch_size: 20 # Number of episodes to train on

# update the target network every {} episodes
target_update_interval_or_tau: 200

save_model: True
save_model_interval: 5000

#checkpoint_path: "D:/MAI/TFM/TFM/results/pistonball_reward_2_actions/qmix/15/models" 
evaluate: False # Evaluate model for test_nepisode episodes and quit (no training)
#load_step: 997800

agent: "rnn" # Default rnn agent
hidden_dim: 64 # Size of hidden state for default rnn agent
obs_agent_id: True # Include the agent's one_hot id in the observation
obs_last_action: True # Include the agent's last action (one_hot) in the observation
obs_individual_obs: False
batch_size_run: 1 # Number of environments to run in parallel

# use the Q_Learner to train
standardise_returns: False
standardise_rewards: False

agent_output_type: "q"
learner: "q_learner"
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

l2_reg_coef: 0

lr_decay: True
lr_decay_gamma: 0.6
lr_decay_steps: 20000

name: "qmix"