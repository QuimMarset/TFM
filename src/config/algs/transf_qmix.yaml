# --- TransformerQMix  ---

name: "transf_qmix"

seed: 0

env_args:
    obs_entity_mode: True
    state_entity_mode: True

# To keep the token dimension fixed, observaion id and last action should not be included
add_agent_id: False
add_last_action: False

runner: "parallel"
batch_size_run: 8
t_max: 2000000
buffer_size: 5000 
batch_size: 5

start_steps: 10000
sigma_start: 0.2
sigma_finish: 0.05
sigma_anneal_time: 2000000

gamma: 0.99

lr: 0.001
critic_lr: 0.001
lr_decay_actor: False
lr_decay_critic: False

grad_norm_clip_actor: 0.5
grad_norm_clip_critic: 0.5

#optimizer_epsilon: 0.01

td_lambda: 0.6

learner: "continuous_transformer_learner"
mac: "transformer_continuous_mac"
agent: "transformer_actor"
mixer: "transformer_continuous"

# parameters of the transformer agent
emb_dim: 16 # embedding dimension of transformer
num_heads: 4 # head number of transformer
num_blocks: 2 # block number of transformer
ff_hidden_mult: 2 # relative dimension of hidden layer of ff after attention
dropout: 0 # multi-head attention dropout
agent_save_memory: False # reduce memory use (at price of time)
use_entity_embeddings: False # learn an embedding for each single entity; will be included also in the mixer
entity_emb_dim: 2

# parameters of the transformer mixer
mixer_emb_dim: 16 # embedding dimension of transformer
mixer_heads: 4 # head number of transformer
mixer_depth: 2 # block number of transformer
qmix_pos_func: "abs" # function to impose monotonic constraint
mixer_save_memory: False # reduce memory use (at price of time)

target_update_mode: 'soft'
hard_update_interval: 200
target_update_tau: 0.005

save_model: False
save_model_interval: 100000

save_model_end: True

actions_regularization: False

use_cuda: True