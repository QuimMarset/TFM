# --- TransformerQMix Discrete ---

name: "transf_qmix_discrete"

#seed: 0

# To keep the token dimension fixed, observaion id and last action should not be included
add_agent_id: False
add_last_action: False

env_args:
    obs_entity_mode: True
    state_entity_mode: True

action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000
evaluation_epsilon: 0

num_envs: 1
buffer_size: 3000
batch_size: 20
t_max: 100000

gamma: 0.99

lr: 0.0005
lr_decay: False
lr_decay_gamma: 0.5
lr_decay_episodes: 500

grad_norm_clip: 500

#optimizer_epsilon: 0.0001

td_lambda: 0.6

learner: "discrete_transformer_learner"
mac: "transformer_mac"
agent: "transformer"
mixer: "transformer"

# parameters of the transformer agent
emb_dim: 32 # embedding dimension of transformer
num_heads: 4 # head number of transformer
num_blocks: 2 # block number of transformer
ff_hidden_mult: 4 # relative dimension of hidden layer of ff after attention
dropout: 0 # multi-head attention dropout

# parameters of the transformer mixer
mixer_emb_dim: 32 # embedding dimension of transformer
mixer_heads: 4 # head number of transformer
mixer_depth: 2 # block number of transformer
qmix_pos_func: "abs" # function to impose monotonic constraint

target_update_mode: 'soft'
hard_update_interval: 200
target_update_tau: 0.005

save_model: False
save_model_interval: 5000

save_model_end: True

use_cuda: True
